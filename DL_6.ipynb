{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* `Import the random module to generate random numbers or make random selections.`\n",
        "* `Import the numpy library as np to utilize its functionality for numerical computations and array manipulation.`"
      ],
      "metadata": {
        "id": "BzL13nc4dL2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAg8Gs8DOWKW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeuralNet class Description\n",
        "1. Initialize neural network parameters: Randomly assign weights and biases for the input-hidden and hidden-output layers.\n",
        "2. Forward propagation:\n",
        "  * Compute the input to the hidden layer by taking the dot product of the input data and the weights, and then adding the biases.\n",
        "  * Apply the Rectified Linear Unit (ReLU) activation function to the hidden layer's input to get the hidden layer's output.\n",
        "  * Compute the input to the output layer by taking the dot product of the hidden layer's output and the weights, and then adding the biases.\n",
        "  * Apply the Sigmoid activation function to the output layer's input to get the predictions.\n",
        "3. Backward propagation:\n",
        "  * Calculate the mean squared error loss between the actual output (y) and the predicted output.\n",
        "  * Compute the error and delta for the output layer.\n",
        "  * Backpropagate the error to the hidden layer by computing the error and delta using the weights of the output layer.\n",
        "4. Update weights and biases:\n",
        "  * Update the weights and biases for the hidden-output layer using gradient descent by multiplying the learning rate with the dot product of the hidden layer's output and the output layer's delta.\n",
        "  * Update the biases for the output layer by multiplying the learning rate with the sum of the output layer's delta.\n",
        "  * Update the weights and biases for the input-hidden layer using gradient descent by multiplying the learning rate with the dot product of the input data and the hidden layer's delta.\n",
        "  * Update the biases for the hidden layer by multiplying the learning rate with the sum of the hidden layer's delta.\n",
        "5. Return the loss, which represents how well the neural network is performing on the given data."
      ],
      "metadata": {
        "id": "sNqpqDt2X7C1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6rQ0gYWBN7Ma"
      },
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.weights_input_hidden = np.random.rand(2, 2)\n",
        "        self.biases_hidden = np.zeros((1, 2))\n",
        "        self.weights_hidden_output = np.random.rand(2, 2)\n",
        "        self.biases_output = np.zeros((1, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input layer to hidden layer\n",
        "        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_hidden\n",
        "        self.hidden_output = relu(self.hidden_input)\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_output\n",
        "        self.predictions = sigmoid(self.output_input)\n",
        "\n",
        "        return self.predictions\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "        # Compute loss\n",
        "        loss = mse_loss(y, self.predictions)\n",
        "\n",
        "        # Backpropagation\n",
        "        output_error = y - self.predictions\n",
        "        output_delta = output_error * sigmoid_derivative(self.output_input)\n",
        "\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * relu_derivative(self.hidden_input)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
        "        self.biases_output += learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
        "        self.weights_input_hidden += learning_rate * np.dot(x.T, hidden_delta)\n",
        "        self.biases_hidden += learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ReLU Function:\n",
        "  * Returns the element-wise maximum of 0 and the input array.\n",
        "2. Sigmoid Function:\n",
        "  * Computes the sigmoid activation function for the input array, which squeezes values between 0 and 1.\n",
        "3. Mean Squared Error (MSE) Loss Function:\n",
        "  * Computes the mean squared error loss between the true output (y_true) and the predicted output (y_pred).\n",
        "4. ReLU Derivative Function:\n",
        "  * Returns the derivative of the ReLU function, which is 1 for input values greater than 0 and 0 otherwise.\n",
        "5. Sigmoid Derivative Function:\n",
        "  * Returns the derivative of the sigmoid function, which is the sigmoid of the input multiplied by (1 - sigmoid of the input)."
      ],
      "metadata": {
        "id": "Os651jSGb1iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "JbA51itcZuBG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define input_data and target_data:\n",
        "  * Create numpy arrays to store the input data and the corresponding target data for training the neural network.\n",
        "2. Create Neural Network Model:\n",
        "  * Instantiate a NeuralNet object to create an instance of the neural network model.\n",
        "3. Set Learning Rate:\n",
        "  * Define the learning rate for the neural network's gradient descent optimization algorithm.\n",
        "4. Training Loop:\n",
        "  * Iterate over a specified number of epochs (1000 in this case).\n",
        "5. Forward Pass:\n",
        "  * Compute predictions for the input data using the neural network model's forward method.\n",
        "6. Backward Pass and Optimization:\n",
        "  * Compute the loss by performing a backward pass through the network and updating weights and biases using gradient descent.\n",
        "7. Print Loss:\n",
        "  * Print the loss value for every 100 epochs to monitor the training progress.\n",
        "Test the Trained Model:\n",
        "8. Define test_input:\n",
        "  * Create a numpy array to store the input data for testing the trained model.\n",
        "9. Predict Output:\n",
        "  * Use the trained model to predict the output for the test input data.\n",
        "10. Print Predicted Output:\n",
        "  * Display the predicted output generated by the trained model for the test input."
      ],
      "metadata": {
        "id": "NvG3-Vr5cNbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = np.array([[0.5, 0.7]])\n",
        "target_data = np.array([[0.3, 0.9]])\n",
        "\n",
        "model = NeuralNet()\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "for epoch in range(1000):\n",
        "    # Forward pass\n",
        "    predictions = model.forward(input_data)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss = model.backward(input_data, target_data, learning_rate)\n",
        "\n",
        "    # Print loss for every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "test_input = np.array([[0.8, 0.4]])\n",
        "predicted_output = model.forward(test_input)\n",
        "print(\"Predicted Output:\", predicted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lilQvrGrY19d",
        "outputId": "abd849d2-84be-4a86-e383-c2d9b8d19f6c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.15960877361027326\n",
            "Epoch 100, Loss: 0.15478317757201135\n",
            "Epoch 200, Loss: 0.15011320667293826\n",
            "Epoch 300, Loss: 0.1455951472215975\n",
            "Epoch 400, Loss: 0.14122519709798298\n",
            "Epoch 500, Loss: 0.13699948123317487\n",
            "Epoch 600, Loss: 0.13291406724954255\n",
            "Epoch 700, Loss: 0.128964981144077\n",
            "Epoch 800, Loss: 0.1251482228489978\n",
            "Epoch 900, Loss: 0.12145978148082165\n",
            "Predicted Output: [[0.62136172 0.55278964]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
